{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1567,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import notebook setup\n",
    "from msc_code.scripts.notebook_setup import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the methods used to collect data from reports, including how many reviewers collected data from each report, whether they worked independently, any processes for obtaining or confirming data from study investigators, and if applicable, details of automation tools used in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1568,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import first author (JGE) screened results as pandas dataframe.\n",
    "import_path = os.path.join(PROC_DATA_DIR, \"full_text_screen\", \"jge_included.csv\")\n",
    "jge_included_df = pd.read_csv(import_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1569,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe of results to proceed with data extraction whilst second author reviews subset of results.\n",
    "jge_data_extraction_df = jge_included_df[['id', 'Publication Year', 'Authors', 'Title', 'Publication Title']].set_index('id')\n",
    "\n",
    "jge_data_extraction_df.to_csv(\"/\".join([PROC_DATA_DIR, 'data_extraction', 'data_extraction_list.csv']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1570,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set import path for data_extraction.xlsx\n",
    "import_path = os.path.join(RAW_DATA_DIR, \"data_extraction\", \"data_extraction.xlsx\")\n",
    "\n",
    "# Import study_data\n",
    "study_data = pd.read_excel(import_path, sheet_name=\"study_data\") # Import study_data\n",
    "patient_data = pd.read_excel(import_path, sheet_name=\"patient_data\") # Import patient_data\n",
    "time_data = pd.read_excel(import_path, sheet_name=\"time_data\") # Import time_data\n",
    "demographic_data = pd.read_excel(import_path, sheet_name=\"demographic_data\") # Import demographic_data\n",
    "object_raw_data = pd.read_excel(import_path, sheet_name=\"object_raw_data\") # Import object_raw_data\n",
    "object_gross_data = pd.read_excel(import_path, sheet_name=\"object_gross_data\") # Import object_raw_data\n",
    "intention_data = pd.read_excel(import_path, sheet_name=\"intention_data\")\n",
    "motivation_data = pd.read_excel(import_path, sheet_name=\"motivation_data\") # Import motivation_data\n",
    "intervention_data = pd.read_excel(import_path, sheet_name=\"intervention_data\") # Import motivation_data\n",
    "outcome_data = pd.read_excel(import_path, sheet_name=\"outcome_data\") # Import outcome_data\n",
    "symptom_data = pd.read_excel(import_path, sheet_name=\"symptom_data\") # Import symptom_data\n",
    "complication_data = pd.read_excel(import_path, sheet_name=\"complication_data\") # Import complication_data\n",
    "incidental_findings_data = pd.read_excel(import_path, sheet_name=\"incidental_findings_data\") # Import incidental_findings_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Object Diameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1571,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2z/pkyt34z93md62kmcdphb_kf00000gn/T/ipykernel_7613/1123714045.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  object_dimensions.replace(\"Unknown\", np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates\n",
    "def calculate_object_diameters(object_raw_data):\n",
    "    if object_raw_data['Object_ID'].duplicated().any():\n",
    "        raise ValueError(\"Duplicate Object_IDs found!\")\n",
    "\n",
    "    object_dimensions = object_raw_data[['Object_ID', 'Object_Length_cm', 'Object_Width_cm', 'Object_Height_cm']].set_index('Object_ID')\n",
    "    object_dimensions.replace(\"Unknown\", np.nan, inplace=True)\n",
    "    object_dimensions = object_dimensions.astype(float)\n",
    "\n",
    "    # Compute Object Diameter\n",
    "    def compute_diameter(row):\n",
    "        available_values = row.dropna()  # Get non-null values in each row\n",
    "        \n",
    "        if len(available_values) == 1:\n",
    "            return available_values.iloc[0]  # If only one dimension value exists, use it directly to compute object diameter\n",
    "        \n",
    "        elif len(available_values) == 2:\n",
    "            return np.sqrt(sum(available_values**2))  # Use Pythagorean theorem if two values exist\n",
    "        \n",
    "        elif len(available_values) == 3:\n",
    "            return max(available_values)  # If all three exist, take the max\n",
    "        \n",
    "        return np.nan  # If no values exist, return NaN\n",
    "\n",
    "    # Apply function row-wise\n",
    "    object_dimensions['Object_Diameter_cm'] = object_dimensions.apply(compute_diameter, axis=1)\n",
    "\n",
    "    object_raw_data[\"Object_Diameter_cm\"] = object_raw_data[\"Object_Diameter_cm\"].fillna(object_raw_data[\"Object_ID\"].map(object_dimensions[\"Object_Diameter_cm\"]))\n",
    "\n",
    "    return object_raw_data\n",
    "\n",
    "# study_data[\"Age_Low_Years\"].fillna(study_data[\"Study_ID\"].map(age_min_map))\n",
    "object_raw_data = calculate_object_diameters(object_raw_data=object_raw_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate study_data from patient_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Sample Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1572,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate sample size\n",
    "def calculate_sample_size(study_data, patient_data):\n",
    "\n",
    "    # Count occurrences of Study_ID in patient_data\n",
    "    patient_count = patient_data['Study_ID'].value_counts()\n",
    "\n",
    "    # Map counts to study_data where Sample_Size is NaN\n",
    "    study_data['Sample_Size'] = study_data.apply(\n",
    "        lambda row: patient_count.get(row['Study_ID'], row['Sample_Size']) if pd.isna(row['Sample_Size']) else row['Sample_Size'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Ensure integer values\n",
    "    study_data['Sample_Size'] = study_data['Sample_Size'].astype(int)\n",
    "\n",
    "    return study_data\n",
    "\n",
    "study_data = calculate_sample_size(study_data=study_data, patient_data=patient_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1573,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2z/pkyt34z93md62kmcdphb_kf00000gn/T/ipykernel_7613/458206785.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  patient_data['Age_Years'] = patient_data['Age_Years'].replace(\"Unknown\", np.nan)\n"
     ]
    }
   ],
   "source": [
    "# Where gross data is unavailable for populations included in a study, data is generated from individual patient data.\n",
    "\n",
    "# Define Function to calculate Age_Min_Years in study_data with data from patient_data where currently blank\n",
    "def calculate_age_low(study_data, patient_data):\n",
    "\n",
    "    # Replace \"Unknown\" values with NaN\n",
    "    patient_data['Age_Years'] = patient_data['Age_Years'].replace(\"Unknown\", np.nan)\n",
    "\n",
    "    # Compute the minimum Age_Years for each Study_ID\n",
    "    age_min_map = patient_data.groupby(\"Study_ID\")[\"Age_Years\"].min()\n",
    "\n",
    "    # Only fill NaN values in Age_Low_Years\n",
    "    study_data[\"Age_Low_Years\"] = study_data[\"Age_Low_Years\"].fillna(study_data[\"Study_ID\"].map(age_min_map))\n",
    "        \n",
    "    return study_data\n",
    "\n",
    "study_data = calculate_age_low(study_data=study_data, patient_data=patient_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1574,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate Age_Max_Years in study_data with data from patient_data where currently blank\n",
    "def calculate_age_high(study_data, patient_data):\n",
    "\n",
    "    # Replace \"Unknown\" values with NaN\n",
    "    patient_data['Age_Years'] = patient_data['Age_Years'].replace(\"Unknown\", np.nan)\n",
    "\n",
    "    # Compute the maximum Age_Years for each Study_ID\n",
    "    age_max_map = patient_data.groupby(\"Study_ID\")[\"Age_Years\"].max()\n",
    "\n",
    "    # Only fill NaN values in Age_High_Years\n",
    "    study_data[\"Age_High_Years\"] = study_data[\"Age_High_Years\"].fillna(study_data[\"Study_ID\"].map(age_max_map))\n",
    "\n",
    "    return study_data\n",
    "\n",
    "study_data = calculate_age_high(study_data=study_data, patient_data=patient_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1575,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate Age_Mean_Years in study_data with data from patient_data where currently blank \n",
    "def calculate_age_mean(study_data, patient_data):\n",
    "\n",
    "    # Replace \"Unknown\" values with NaN\n",
    "    patient_data['Age_Years'] = patient_data['Age_Years'].replace(\"Unknown\", np.nan)\n",
    "\n",
    "    # Compute the maximum Age_Years for each Study_ID\n",
    "    age_mean_map = patient_data.groupby(\"Study_ID\")[\"Age_Years\"].mean()\n",
    "\n",
    "    # Only fill NaN values in Age_High_Years\n",
    "    study_data[\"Age_Mean_Years\"] = study_data[\"Age_Mean_Years\"].fillna(study_data[\"Study_ID\"].map(age_mean_map))\n",
    "\n",
    "    return study_data\n",
    "\n",
    "study_data = calculate_age_mean(study_data=study_data, patient_data=patient_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1576,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate N_Female\n",
    "def calculate_gender_counts(study_data, patient_data):\n",
    "    # Ensure Gender values are consistent\n",
    "    valid_genders = [\"Male\", \"Female\", \"Unknown\"]\n",
    "    patient_data = patient_data[patient_data[\"Gender\"].isin(valid_genders)]\n",
    "\n",
    "    # Compute gender counts per Study_ID\n",
    "    gender_counts = patient_data.groupby(\"Study_ID\")[\"Gender\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Ensure all expected columns exist\n",
    "    for gender in valid_genders:\n",
    "        if gender not in gender_counts.columns:\n",
    "            gender_counts[gender] = 0  # Add missing gender columns\n",
    "\n",
    "    # Map gender counts directly to the corresponding columns in study_data\n",
    "    study_data[\"N_Gender_Male\"] = study_data[\"Study_ID\"].map(gender_counts.get(\"Male\", {})).fillna(0).astype(int)\n",
    "    study_data[\"N_Gender_Female\"] = study_data[\"Study_ID\"].map(gender_counts.get(\"Female\", {})).fillna(0).astype(int)\n",
    "    study_data[\"N_Gender_Unknown\"] = study_data[\"Study_ID\"].map(gender_counts.get(\"Unknown\", {})).fillna(0).astype(int)\n",
    "\n",
    "    return study_data\n",
    "\n",
    "# Apply function\n",
    "study_data = calculate_gender_counts(study_data=study_data, patient_data=patient_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1577,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to calculate mortality rate in study_data from outcome_data\n",
    "def calculate_mortality_count(study_data, outcome_data):\n",
    "    # Ensure mortality recording is consistent\n",
    "    valid_mortality_outcomes = [\"Yes\", \"No\", \"Unknown\"]\n",
    "    outcome_data = outcome_data[outcome_data[\"Mortality\"].isin(valid_mortality_outcomes)]\n",
    "\n",
    "    # Drop duplicate Patient_ID to ensure each patient is counted only once\n",
    "    unique_mortality = outcome_data.drop_duplicates(subset=\"Patient_ID\")\n",
    "\n",
    "    # Compute mortality counts per Study_ID\n",
    "    mortality_counts = unique_mortality[unique_mortality[\"Mortality\"] == \"Yes\"].groupby(\"Study_ID\")[\"Patient_ID\"].count()\n",
    "\n",
    "    # Map mortality counts to study_data (fill NaN with 0)\n",
    "    study_data[\"Mortality_Count\"] = study_data[\"Study_ID\"].map(mortality_counts).fillna(0).astype(int)\n",
    "\n",
    "    # Replace zeros with NaN to avoid division by zero errors, then compute Mortality Rate\n",
    "    study_data[\"Mortality_Rate\"] = np.where(\n",
    "        study_data[\"Sample_Size\"] > 0, \n",
    "        study_data[\"Mortality_Count\"] / study_data[\"Sample_Size\"], \n",
    "        np.nan  # Set to NaN if division by zero would occur\n",
    "    )\n",
    "\n",
    "    return study_data\n",
    "\n",
    "# Apply function\n",
    "study_data = calculate_mortality_count(study_data=study_data, outcome_data=outcome_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate complication rate\n",
    "def calculate_complication_rate(study_data, complication_data):\n",
    "\n",
    "    pass\n",
    "\n",
    "# study_data = calculate_complication_rate(study_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate intentional counts\n",
    "def calculate_intention_counts(intention_data, motivation_data):\n",
    "\n",
    "    intention_map = motivation_data.groupby(\"Study_ID\")[\"Intentional\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    # Create a mapping series for each category while ensuring missing keys return 0\n",
    "    intentional_map = intention_map.get(\"Yes\", pd.Series(0, index=intention_data[\"Study_ID\"]))\n",
    "    non_intentional_map = intention_map.get(\"No\", pd.Series(0, index=intention_data[\"Study_ID\"]))\n",
    "    unknown_map = intention_map.get(\"Unknown\", pd.Series(0, index=intention_data[\"Study_ID\"]))\n",
    "\n",
    "    # Only overwrite NaN values\n",
    "    intention_data[\"N_Intentional_Ingestion\"] = intention_data[\"N_Intentional_Ingestion\"].fillna(intention_data[\"Study_ID\"].map(intentional_map))\n",
    "    intention_data[\"N_Non_Intentional_Ingestion\"] = intention_data[\"N_Non_Intentional_Ingestion\"].fillna(intention_data[\"Study_ID\"].map(non_intentional_map))\n",
    "    intention_data[\"N_Unknown_Intention\"] = intention_data[\"N_Unknown_Intention\"].fillna(intention_data[\"Study_ID\"].map(unknown_map))\n",
    "\n",
    "    return intention_data\n",
    "\n",
    "intention_data = calculate_intention_counts(intention_data=intention_data, motivation_data=motivation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1580,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Psychiatric History count\n",
    "def calculate_n_psych_history(demographic_data, patient_data):\n",
    "\n",
    "    psych_history_data = patient_data.groupby(\"Study_ID\")[\"Psychiatric_History\"].value_counts().unstack(fill_value=0)\n",
    "\n",
    "    psych_history_map = psych_history_data.get(\"Yes\", pd.Series(0, index=demographic_data[\"Study_ID\"]))\n",
    "\n",
    "    demographic_data[\"N_Psych_History\"] = demographic_data[\"N_Psych_History\"].fillna(demographic_data[\"Study_ID\"].map(psych_history_map))\n",
    "\n",
    "    return demographic_data\n",
    "\n",
    "demographic_data = calculate_n_psych_history(demographic_data=demographic_data, patient_data=patient_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1581,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_time_ingestion_to_presentation\n",
    "def calculate_time_ingestion_to_presentation(object_raw_data):\n",
    "\n",
    "    # Conversion factors\n",
    "    time_multipliers = {\n",
    "        'hours': 1,\n",
    "        'days': 24,\n",
    "        'weeks': 7 * 24,\n",
    "        'months': 30.44 * 24,  # Average month length in hours\n",
    "        'years': 365.25 * 24   # Account for leap years\n",
    "    }\n",
    "\n",
    "    # Function to convert time expressions to hours\n",
    "    def convert_to_hours(time_str):\n",
    "        if isinstance(time_str, str) and time_str.lower() == \"unknown\":\n",
    "            return np.nan  # Keep unknowns as NaN\n",
    "\n",
    "        match = re.match(r\"([\\d\\.]+)(\\w+)\", str(time_str))\n",
    "        if match:\n",
    "            value, unit = match.groups()\n",
    "            value = float(value)  # Convert number part to float\n",
    "            if unit in time_multipliers:\n",
    "                return value * time_multipliers[unit]  # Convert to hours\n",
    "        return np.nan  # If it doesn't match expected format\n",
    "\n",
    "    # Apply conversion, only overwriting NaN values\n",
    "    object_raw_data[\"Time_Ingestion_To_Presentation_Hrs\"] = object_raw_data[\"Time_Ingestion_To_Presentation_Hrs\"].fillna(\n",
    "        object_raw_data[\"Time_In_Situ_At_Presentation_Long\"].apply(convert_to_hours)\n",
    "    )\n",
    "\n",
    "    return object_raw_data\n",
    "\n",
    "object_raw_data = calculate_time_ingestion_to_presentation(object_raw_data=object_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Time to Ingestion Categories\n",
    "def calculate_time_to_presentation_type(object_raw_data):\n",
    "    \"\"\"\n",
    "    Categorises ingestion time into different time types.\n",
    "    \n",
    "    Parameters:\n",
    "    - object_data (pd.DataFrame): DataFrame containing 'Time_Ingestion_To_Presentation_Hrs'\n",
    "    \n",
    "    Returns:\n",
    "    - object_data (pd.DataFrame): Updated DataFrame with 'Time_Ingestion_To_Presentation_Type'\n",
    "    \"\"\"\n",
    "\n",
    "    # Define bins and labels\n",
    "    bins = [0, 12, 48, np.inf]\n",
    "    labels = [\"<12hrs\", \"12-48hrs\", \">48hrs\"]\n",
    "\n",
    "    # Categorize using pd.cut\n",
    "    object_raw_data[\"Time_Ingestion_To_Presentation_Type\"] = pd.cut(\n",
    "        object_raw_data[\"Time_Ingestion_To_Presentation_Hrs\"], bins=bins, labels=labels, include_lowest=True\n",
    "    ).astype(str)  # Convert to string to allow \"Unknown\"\n",
    "\n",
    "    # Assign \"Unknown\" where ingestion time is NaN\n",
    "    object_raw_data.loc[object_raw_data[\"Time_Ingestion_To_Presentation_Hrs\"].isna(), \"Time_Ingestion_To_Presentation_Type\"] = \"Unknown\"\n",
    "\n",
    "    return object_raw_data\n",
    "\n",
    "# Apply function to object_data\n",
    "object_raw_data = calculate_time_to_presentation_type(object_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1583,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time ingestion to intervention\n",
    "def calculate_time_ingestion_to_intervention(intervention_data):\n",
    "\n",
    "    # Conversion factors\n",
    "    time_multipliers = {\n",
    "        'hours': 1,\n",
    "        'days': 24,\n",
    "        'weeks': 7 * 24,\n",
    "        'months': 30.44 * 24,  # Average month length in hours\n",
    "        'years': 365.25 * 24   # Account for leap years\n",
    "    }\n",
    "\n",
    "    # Function to convert time expressions to hours\n",
    "    def convert_to_hours(time_str):\n",
    "        if isinstance(time_str, str) and time_str.lower() == \"unknown\":\n",
    "            return np.nan  # Keep unknowns as NaN\n",
    "\n",
    "        match = re.match(r\"([\\d\\.]+)(\\w+)\", str(time_str))\n",
    "        if match:\n",
    "            value, unit = match.groups()\n",
    "            value = float(value)  # Convert number part to float\n",
    "            if unit in time_multipliers:\n",
    "                return value * time_multipliers[unit]  # Convert to hours\n",
    "        return np.nan  # If it doesn't match expected format\n",
    "\n",
    "    # Apply conversion, only overwriting NaN values\n",
    "    intervention_data[\"Time_Ingestion_To_Intervention_Hrs\"] = intervention_data[\"Time_Ingestion_To_Intervention_Hrs\"].fillna(\n",
    "        intervention_data[\"Time_Ingestion_To_Intervention_Long\"].apply(convert_to_hours)\n",
    "    )\n",
    "\n",
    "    return intervention_data\n",
    "\n",
    "intervention_data = calculate_time_ingestion_to_intervention(intervention_data=intervention_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1584,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_n_time_ingestion_to_presentation(object_raw_data, time_data):\n",
    "    # valid_time_bins = [\"<12hrs\", \"12-48hrs\", \">48hrs\"]\n",
    "\n",
    "    time_window_map = object_raw_data.groupby(\"Study_ID\")[\"Time_Ingestion_To_Presentation_Type\"].value_counts().unstack(fill_value=0)\n",
    "    \n",
    "    time_data[\"N_Time_Ingestion_To_Presentation_<12hrs\"] = time_data[\"N_Time_Ingestion_To_Presentation_<12hrs\"].fillna(time_data[\"Study_ID\"].map(time_window_map[\"<12hrs\"]))\n",
    "    time_data[\"N_Time_Ingestion_To_Presentation_12-48hrs\"] = time_data[\"N_Time_Ingestion_To_Presentation_12-48hrs\"].fillna(time_data[\"Study_ID\"].map(time_window_map[\"12-48hrs\"]))\n",
    "    time_data[\"N_Time_Ingestion_To_Presentation_48hrs+\"] = time_data[\"N_Time_Ingestion_To_Presentation_48hrs+\"].fillna(time_data[\"Study_ID\"].map(time_window_map[\">48hrs\"]))\n",
    "    time_data[\"N_Time_Ingestion_To_Presentation_Unknown\"] = time_data[\"N_Time_Ingestion_To_Presentation_Unknown\"].fillna(time_data[\"Study_ID\"].map(time_window_map[\"Unknown\"]))\n",
    "    return time_data\n",
    "\n",
    "time_data = calculate_n_time_ingestion_to_presentation(object_raw_data=object_raw_data, time_data=time_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorise object initial location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1586,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorise object final location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_n_successful_endoscopy()\n",
    "\n",
    "    # Extraction success with endoscopy was defined as complete removal of the foreign bodies without complications or surgical intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1588,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all data\n",
    "study_data.to_csv(\"/\".join([PROC_DATA_DIR, \"data_extraction\", \"study_data.csv\"]), index=False)\n",
    "patient_data.to_csv(\"/\".join([PROC_DATA_DIR, \"data_extraction\", \"patient_data.csv\"]), index=False)\n",
    "time_data.to_csv(\"/\".join([PROC_DATA_DIR, \"data_extraction\", \"time_data.csv\"]), index=False)\n",
    "demographic_data.to_csv(\"/\".join([PROC_DATA_DIR, \"data_extraction\", \"demographic_data.csv\"]), index=False)\n",
    "object_raw_data.to_csv(\"/\".join([PROC_DATA_DIR, \"data_extraction\", \"object_raw_data.csv\"]), index=False)\n",
    "object_gross_data.to_csv(\"/\".join([PROC_DATA_DIR, \"data_extraction\", \"object_gross_data.csv\"]), index=False)\n",
    "intention_data.to_csv(\"/\".join([PROC_DATA_DIR, \"data_extraction\", \"intention_data.csv\"]), index=False)\n",
    "motivation_data.to_csv(\"/\".join([PROC_DATA_DIR, \"data_extraction\", \"motivation_data.csv\"]), index=False)\n",
    "intervention_data.to_csv(\"/\".join([PROC_DATA_DIR, \"data_extraction\", \"intervention_data.csv\"]), index=False)\n",
    "outcome_data.to_csv(\"/\".join([PROC_DATA_DIR, \"data_extraction\", \"outcome_data.csv\"]), index=False)\n",
    "symptom_data.to_csv(\"/\".join([PROC_DATA_DIR, \"data_extraction\", \"symptom_data.csv\"]), index=False)\n",
    "complication_data.to_csv(\"/\".join([PROC_DATA_DIR, \"data_extraction\", \"complication_data.csv\"]), index=False)\n",
    "incidental_findings_data.to_csv(\"/\".join([PROC_DATA_DIR, \"data_extraction\", \"incidental_findings_data.csv\"]), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
