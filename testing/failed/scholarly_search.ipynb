{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Google Scholar with Scholarly\n",
    "\n",
    "**Author:** Jack Galbraith-Edge\n",
    "**Date:** 8th January 2025\n",
    "\n",
    "## Introduction\n",
    "Google Scholar is popular tool for searching grey literature. Unlike databases like PubMed and Embase which are curated, maintained by humans and contain only published literature. Google Scholar, on the other hand, searches the internet for what it believes are academic materials and then displays them in a list. As a result, Google Scholar is excellent for seaching grey literature, as results will include unpublished data such as thesises and other useful material.\n",
    "\n",
    "Unfortunately, because Google Scholar references the internet in this way and is not itself a database, this can make querying cumbersome and the number of results returned by queries and be overwhelming. \n",
    "\n",
    "In this document, I intend to use the python library 'Scholarly' to query and scrape information from Google Scholar. I will then manipulate this data into a useable format for use in my systematic review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from scholarly import scholarly, ProxyGenerator\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because scraping information from Google Scholar is technically against the google scholar licence agreement, a proxy is used. Otherwise, google will simply block IPs that it believe to be are bots, or are working at super human speed with their queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free proxy set up. Test beginning shortly.\n",
      "Error during query execution: Cannot Fetch from Google Scholar.\n"
     ]
    }
   ],
   "source": [
    "# initialise proxy generator object\n",
    "pg = ProxyGenerator()\n",
    "\n",
    "# set up a proxy with free proxy\n",
    "success = pg.FreeProxies()\n",
    "\n",
    "# if proxy successfully deployed\n",
    "if success:\n",
    "    scholarly.use_proxy(pg)\n",
    "    print(\"Free proxy set up. Test beginning shortly.\")\n",
    "    pg_dict = (pg.__dict__) # print pg dictionary object\n",
    "\n",
    "    proxies = pg_dict.get(\"_proxies\")\n",
    "\n",
    "    # try to run a test query\n",
    "    try:\n",
    "        test_result = scholarly.search_pubs(\"foreign body ingestion\") # run a test query\n",
    "        first_result = next(test_result) # get first result\n",
    "        scholarly.pprint(first_result) # print first result\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query execution: {e}\")\n",
    "else:\n",
    "    print(\"Failed to set up proxy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Proxy: http://168.234.75.168:80, HTTPS Proxy: http://168.234.75.168:80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:1100: InsecureRequestWarning: Unverified HTTPS request is being made to host '168.234.75.168'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy is working! Response received successfully.\n",
      "Running test query...\n",
      "Error during test query execution: Cannot Fetch from Google Scholar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialise_free_proxy():\n",
    "    \n",
    "    # initialise proxy generator object\n",
    "    pg = ProxyGenerator()\n",
    "\n",
    "    try:\n",
    "        # set up a proxy with free proxy\n",
    "        success = pg.FreeProxies()\n",
    "\n",
    "        # check if proxy set up correctly\n",
    "        if success:\n",
    "            # get internal dictionary of ProxyGenerator object\n",
    "            pg_dict = (pg.__dict__) \n",
    "            proxies = pg_dict.get(\"_proxies\")\n",
    "\n",
    "            # store proxy details in a dictionary\n",
    "            proxy_dict = {\n",
    "                'http': proxies['http://'],\n",
    "                'https': proxies['https://']\n",
    "            }\n",
    "\n",
    "\n",
    "            # print proxies to stdout\n",
    "            print(f\"HTTP Proxy: {proxy_dict['http']}, HTTPS Proxy: {proxy_dict['https']}\")\n",
    "\n",
    "\n",
    "            # check proxies are working by making a request to google scholar \n",
    "            \n",
    "            # make request\n",
    "            response = requests.get(\"https://scholar.google.com\", proxies=proxy_dict, verify=False, timeout=10)\n",
    "\n",
    "            # check response status\n",
    "            if response.status_code == 200:\n",
    "                print(\"Proxy is working! Response received successfully.\")\n",
    "                print(\"Running test query...\")\n",
    "\n",
    "                # after successful request, run test query\n",
    "                try:\n",
    "                    scholarly.use_proxy(pg)\n",
    "                    search_results = scholarly.search_pubs(\"machine learning\")\n",
    "                    first_result = next(search_results)\n",
    "                    scholarly.pprint(first_result)  # print first result\n",
    "                    return True, pg # proxy successfully initialised\n",
    "                # if error making query, print exception.    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error during test query execution: {e}\")\n",
    "                    return False\n",
    "\n",
    "            # if unable to make request fails, print response status code\n",
    "            else:\n",
    "                print(f\"Proxy test failed with status code: {response.status_code}\")\n",
    "                return False\n",
    "\n",
    "        else: \n",
    "            print(\"Failed to set up free proxy.\")\n",
    "            return False\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during proxy initialisation: {e}\")\n",
    "        return False\n",
    "\n",
    "initialise_free_proxy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the error 'Cannot Fetch from Google Scholar'. Kept appearing for me. \n",
    "I tried with multiple free proxies, but they kept failing, even with short queries like 'foreign body ingestion'. \n",
    "\n",
    "Paid proxies cost upwards of $50 per month, so I sought other avenues to achieve this.\n",
    "\n",
    "I opted to change tactic and use a different library from SerpAPI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
