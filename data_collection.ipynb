{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Literature Search\n",
    "- To run a literature search of PubMed, Embase, CENTRAL, Web of Science, SCOPUS, PsycINFO\n",
    "\n",
    "## Queries\n",
    "\n",
    "### Pubmed Query:\n",
    "(\"Foreign Bodies\"[MeSH] OR \"foreign obj*” OR \"foreign bod*”)\n",
    "AND\n",
    "(\"Self-Injurious Behavior[MeSH]\" OR \"intent*” OR \"deliberate*\" OR \"purpose*” OR \"self-injur*” OR \"selfharm*\" OR \"self-harm*”)\n",
    "AND\n",
    "(“ingest*” OR “swallow*”))\n",
    "AND\n",
    "(“Endoscopy”[MeSH] OR “Surgical Procedures, Operative”[MeSH] OR “Conservative Treatment”[MeSH] OR \"Drug Therapy\"[MeSH] OR “surg*” OR “endoscop*” OR “EGD” OR “OGD” OR “Esophagogastroduodenoscopy” OR “Oesophagogastroduodenoscopy” OR “manag*”)\n",
    "\n",
    "### Embase Query\n",
    "All Fields\n",
    "\n",
    "('foreign body'/exp OR 'foreign obj*' OR 'foreign bod*') \n",
    "AND \n",
    "('automutilation'/exp OR 'intent*' OR 'deliberate*' OR 'purpose*' OR 'self-injur*' OR 'selfharm*' OR 'self-harm*') \n",
    "AND \n",
    "('swallowing'/exp OR 'ingest*' OR 'swallow*') \n",
    "AND \n",
    "('endoscopy'/exp OR 'surgery'/exp OR 'conservative treatment'/exp OR 'drug therapy'/exp OR 'surg*' OR 'endoscop*' OR “EGD” OR “OGD” OR “Esophagogastroduodenoscopy” OR “Oesophagogastroduodenoscopy” OR 'manag*')\n",
    "\n",
    "### Cochrane (CENTRAL) Query\n",
    "All Fields\n",
    "\n",
    "([mh foreign bodies] OR (foreign NEXT obj*) OR (foreign NEXT bod*))\n",
    "AND\n",
    "([mh self-injurious behavior] OR intent* OR deliberate* OR purpose* OR (self NEXT injur*) OR (self NEXT harm*))\n",
    "AND\n",
    "(ingest* OR swallow*)\n",
    "AND\n",
    "([mh endoscopy\"] OR [mh \"surgical procedures, operative\"] OR [mh \"conservative treatment\"] OR [mh \"drug therapy\"] OR 'surg*' OR 'endoscop*' OR “EGD” OR “Esophagogastroduodenoscopy” OR “Oesophagogastroduodenoscopy” OR 'manag*'))\n",
    "\n",
    "\n",
    "### Web of Science Query\n",
    "Link: https://www.webofscience.com/wos/woscc/summary/4da44d48-3e09-4a94-a3bd-ff8139e94859-01387ccd63/relevance/1\n",
    "\n",
    "(ALL=(foreign obj* OR foreign bod*)\n",
    "AND\n",
    "ALL=(automutilation OR intent* OR deliberate* OR purpose* OR self-injur* OR selfharm* OR self-harm*)\n",
    "AND \n",
    "ALL=(swallowing OR ingest* OR swallow*)\n",
    "AND\n",
    "ALL=(endoscopy OR surgery OR conservative treatment OR drug therapy OR surg* OR endoscop* OR EGD OR Esophagogastroduodenoscopy OR Oesophagogastroduodenoscopy OR manag*))\n",
    "\n",
    "### SCOPUS Query\n",
    "( ALL ( foreign PRE/0 obj* OR foreign PRE/0 bod* ) \n",
    "AND ALL ( intent* OR deliberate* OR purpose* OR self PRE/0 injur* OR self PRE/0 harm* ) \n",
    "AND ALL ( ingest* OR swallow* ) \n",
    "AND ALL ( endoscopy OR surgery OR ‘conservative’ OR ‘treatment’ OR ‘drug’ OR ‘therapy’ OR surg* OR endoscop* OR egd OR esophagogastroduodenoscopy OR oesophagogastroduodenoscopy OR manag* ) )\n",
    "\n",
    "### PsycINFO\n",
    "Link: https://search-ebscohost-com.ezproxy.library.qmul.ac.uk/login.aspx?direct=true&db=psyh&bquery=(foreign+obj*+OR+foreign+bod*)+AND+(DE+%26quot%3bNonsuicidal+Self-Injury%26quot%3b+OR+automutilation+OR+intent*+OR+deliberate*+OR+purpose*+OR+self+injur*+OR+self+harm*)+AND+(DE+%26quot%3bIngestion%26quot%3b+OR+ingest*+OR+swallow*)+AND+(DE+%26quot%3bSurgery%26quot%3b+OR+endoscop*+OR+conservative+treatment+OR+drug+therapy+surg*+OR+endoscop*+OR+egd+OR+esophagogastroduodenoscopy+OR+oesophagogastroduodenoscopy+OR+manag*)&type=0&searchMode=Standard&site=ehost-live\n",
    "\n",
    "(foreign obj* OR foreign bod*)\n",
    "AND\n",
    "(DE \"Nonsuicidal Self-Injury\" OR automutilation OR intent* OR deliberate* OR purpose* OR self injur* OR self harm*)\n",
    "AND\n",
    "(DE \"Ingestion\" OR ingest* OR swallow*)\n",
    "AND\n",
    "(DE \"Surgery\" OR endoscop* OR conservative treatment OR drug therapy surg* OR endoscop* OR egd OR esophagogastroduodenoscopy OR oesophagogastroduodenoscopy OR manag*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python libraries\n",
    "import os\n",
    "import pandas as pd     # for dataframe functionality\n",
    "from dotenv import load_dotenv, dotenv_values # for environmental variables\n",
    "\n",
    "# Import custom functions from helper.py file for use in this project.\n",
    "from helpers import dataframe_to_ris, track_duplicate_removal\n",
    "\n",
    "# Try to load environment variables from .env file\n",
    "try: \n",
    "    load_dotenv()   # should return True if successful.\n",
    "except ImportError:\n",
    "    # if fails, print error.\n",
    "    print(\"Environment Variables failed to load\")\n",
    "\n",
    "INPUT_PATH = os.getenv('INPUT_PATH')    # define input path for environment\n",
    "OUTPUT_PATH = os.getenv('OUTPUT_PATH')  # define output path for environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatination of Database Datframes and Duplicate Removal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data from database queries\n",
    "- As panadas dataframes, into data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionary to store search data in\n",
    "data = {}\n",
    "\n",
    "# import dataframes\n",
    "data[\"pubmed\"] = pd.read_csv(\"/\".join([INPUT_PATH, \"pubmed.csv\"]))          # PubMed\n",
    "data[\"embase\"] = pd.read_csv(\"/\".join([INPUT_PATH, \"embase.csv\"]))          # Embase\n",
    "data[\"cochrane\"] = pd.read_csv(\"/\".join([INPUT_PATH, \"cochrane.csv\"]))      # Cochrane\n",
    "data[\"wos\"] = pd.read_excel(\"/\".join([INPUT_PATH, \"wos.xls\"]))              # WOS\n",
    "data[\"scopus\"] = pd.read_csv(\"/\".join([INPUT_PATH, \"scopus.csv\"]))          # SCOPUS        \n",
    "data[\"psycinfo\"] = pd.read_csv(\"/\".join([INPUT_PATH, \"psycinfo.csv\"]))      # PyscINFO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track Paper Count / Duplicate Removal Tracking\n",
    "- Use a python dictionary to track the number of papers present i.e. to track the number of duplicate removals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define paper count dict \n",
    "paper_count_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise Dataframe Column Headers.\n",
    "- Dataframes imported from database searches contain numerous fields, and they differ between databases.\n",
    "- The columns to have in the final dataframe that will be used for title and abstract screening to standardise the data.\n",
    "- In this stage, I will rename certain columns in specific database dataframes and then concatinate the dataframes into one final dataframe for title and abstract screening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns I want in my final dataframe are: ['Publication Year', 'First Author', 'Authors', 'Publication Title', 'Title', 'Abstract', 'DOI', 'Database']\n"
     ]
    }
   ],
   "source": [
    "# define desired column headers for later extraction to final dataframe\n",
    "desired_columns = [\n",
    "    \"Publication Year\",\n",
    "    \"First Author\",\n",
    "    \"Authors\",\n",
    "    \"Publication Title\",\n",
    "    \"Title\",\n",
    "    \"Abstract\",\n",
    "    \"DOI\",\n",
    "    \"Database\"\n",
    "]\n",
    "\n",
    "print(f\"The columns I want in my final dataframe are: {desired_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename column headers\n",
    "- For each database dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pubmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename pubmed dataframe columns to fit desired column headers \n",
    "data[\"pubmed\"].rename(columns={     # rename columns\n",
    "    \"Abstract Note\": \"Abstract\",    # rename 'Abstract Note' as 'Abstract'\n",
    "    \"Key\": \"PMID\",                  # rename 'Key' as 'PMID'\n",
    "    \"Author\": \"Authors\"             # rename 'Author' as 'Authors'\n",
    "}, inplace=True)                    # keep changes in place on original dataframe\n",
    "\n",
    "# create first author column formatted as Surname, F.\n",
    "data[\"pubmed\"][\"First Author\"] = (\n",
    "    data[\"pubmed\"][\"Authors\"]\n",
    "    .fillna(\"\")                     # Replace NaN with an empty string\n",
    "    .str.split(\";\")                 # Split multiple authors by ';'\n",
    "    .str[0]                         # Take the first author\n",
    "    .str.strip()                    # Remove leading/trailing spaces\n",
    "    .apply(lambda name: f\"{name.split()[0]} {name.split()[1][0]}.\" if len(name.split()) > 1 else name)\n",
    ")   # format final First Author string as Surname, F."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename embase dataframe columns to fit desired column headers\n",
    "data[\"embase\"].rename(columns={             # rename columns\n",
    "    \"Medline PMID\": \"PMID\",                 # 'Medline PMID' as 'PMID'\n",
    "    \"Source title\": \"Publication Title\",    # 'Source title' as 'Publication Title'\n",
    "    \"Author Names\": \"Authors\"               # 'Author Names' as 'Authors'\n",
    "}, inplace=True)    \n",
    "\n",
    "# create first author column formatted as Surname, F.\n",
    "data[\"embase\"][\"First Author\"] = (\n",
    "    data[\"embase\"][\"Authors\"]\n",
    "    .fillna(\"\")  # Replace NaN with an empty string\n",
    "    .str.split(\";\")  # Split multiple authors by ';'\n",
    "    .str[0]  # Take the first author\n",
    "    .str.strip()  # Remove leading/trailing spaces\n",
    "    .apply(\n",
    "        lambda name: f\"{name.split()[0]}, {name.split()[1][0]}.\" \n",
    "        if len(name.split()) > 1 else name\n",
    "    )   # format final First Author string as Surname, F.\n",
    "    .str.rstrip(\",\")  # Remove trailing commas if present\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cochrane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename cochrane columns to fit desired column headers\n",
    "data[\"cochrane\"].rename(columns={           # Rename columns\n",
    "    \"Year\": \"Publication Year\",             # 'Year' as 'Publication Year'\n",
    "    \"Source\": \"Publication Title\",          # 'Source' as 'Publication Title'\n",
    "    \"Author(s)\": \"Authors\"                  # 'Author(s) as 'Authors'\n",
    "}, inplace=True)                            # in place on original dataframe\n",
    "\n",
    "# create first author column formatted as Surname, F.\n",
    "data[\"cochrane\"][\"First Author\"] = (\n",
    "    data[\"cochrane\"][\"Authors\"]\n",
    "    .fillna(\"\")  # Replace NaN with an empty string\n",
    "    .str.split(\";\")  # Split multiple authors by ';'\n",
    "    .str[0]  # Take the first author\n",
    "    .str.strip()  # Remove leading/trailing spaces\n",
    "    .apply(\n",
    "        lambda name: f\"{name.split()[0]} {name.split()[1][0]}.\" \n",
    "        if len(name.split()) > 1 else name\n",
    "    )   # format final First Author string as Surname, F.\n",
    "    .str.rstrip(\",\")  # Remove trailing commas if present\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web of Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Web of Science columns to fit desired column headers\n",
    "data[\"wos\"].rename(columns={                # rename columns\n",
    "    \"Pubmed Id\": \"PMID\",                    # 'Pubmed Id' as 'PMID'\n",
    "    \"Article Title\": \"Title\",               # 'Article Title' as 'Title'\n",
    "    \"Source Title\": \"Publication Title\"     # 'Source Title' as 'Publication Title'\n",
    "}, inplace=True)                            # Save changes in place on original dataframe\n",
    "\n",
    "# create first author column formatted as Surname, F.\n",
    "data[\"wos\"][\"First Author\"] = (\n",
    "    data[\"wos\"][\"Authors\"]\n",
    "    .fillna(\"\")  # Replace NaN with an empty string\n",
    "    .str.split(\";\")  # Split multiple authors by ';'\n",
    "    .str[0]  # Take the first author\n",
    "    .str.strip()  # Remove leading/trailing spaces\n",
    "    .apply(\n",
    "        lambda name: f\"{name.split()[0]} {name.split()[1][0]}.\" \n",
    "        if len(name.split()) > 1 else name\n",
    "    )   # format final First Author string as Surname, F.\n",
    "    .str.rstrip(\",\")  # Remove trailing commas if present\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCOPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename wos columns to fit desired column headers\n",
    "data[\"scopus\"].rename(columns={\n",
    "    \"Year\": \"Publication Year\",\n",
    "    \"PubMed ID\": \"PMID\",\n",
    "    \"Source title\": \"Publication Title\"\n",
    "}, inplace=True)\n",
    "\n",
    "# create first author column formatted as Surname, F.\n",
    "data[\"scopus\"][\"First Author\"] = (\n",
    "    data[\"scopus\"][\"Authors\"]\n",
    "    .fillna(\"\")  # Replace NaN with an empty string\n",
    "    .str.split(\";\")  # Split multiple authors by ';'\n",
    "    .str[0]  # Take the first author\n",
    "    .str.strip()  # Remove leading/trailing spaces\n",
    "    .apply(\n",
    "        lambda name: f\"{name.split()[0]}, {name.split()[1][0]}.\" \n",
    "        if len(name.split()) > 1 else name\n",
    "    )\n",
    "    .str.rstrip(\",\")  # Remove trailing commas if present\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PsycINFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename PsycINFO column headers to fit desired column headers.\n",
    "data['psycinfo'].rename(columns={           # rename columns\n",
    "    \"Abstract Note\": \"Abstract\",            # 'Abstract Note' as 'Abstract'\n",
    "    \"Author\": \"Authors\"                     # 'Author' as 'Authors'\n",
    "}, inplace=True)                            # save changes in place on original dataframe.\n",
    "\n",
    "# Create first author column formatted as Surname, F.\n",
    "data[\"psycinfo\"][\"First Author\"] = (      \n",
    "    data[\"psycinfo\"][\"Authors\"]\n",
    "    .fillna(\"\")  # Replace NaN with an empty string\n",
    "    .str.split(\";\")  # Split multiple authors by ';'\n",
    "    .str[0]  # Take the first author\n",
    "    .str.strip()  # Remove leading/trailing spaces\n",
    "    .apply(\n",
    "        lambda name: f\"{name.split()[0]} {name.split()[1][0]}.\" \n",
    "        if len(name.split()) > 1 else name\n",
    "    )   # format final First Author string as Surname, F.\n",
    "    .str.rstrip(\",\")  # Remove trailing commas if present\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Database column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Database column to each dataframe so I know which database each source came from.\n",
    "for key, df in data.items():\n",
    "    if key == \"pubmed\":\n",
    "        df['Database'] = \"PubMed\"\n",
    "    elif key == \"embase\":\n",
    "        df['Database'] = \"Embase\"\n",
    "    elif key == \"cochrane\":\n",
    "        df['Database'] = \"Cochrane\"\n",
    "    elif key == \"wos\":\n",
    "        df['Database'] = \"Web of Science\"\n",
    "    elif key == \"scopus\":\n",
    "        df['Database'] = \"SCOPUS\"\n",
    "    elif key == \"psycinfo\":\n",
    "        df['Database'] = \"PsycINFO\"\n",
    "    else:\n",
    "        df['Database'] = \"Unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Final Dataframe\n",
    "- Combine desired columns from each database into one large dataframe for analysis and export for Title and Abstract Screening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract desired columns for database dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = [] # create list in which to store extracted data.\n",
    "for database, df in data.items():       # iterate through each database and corresponding dataframe in data dictionary\n",
    "    extracted_df = df[desired_columns]  # select predefined desired columns from selected dataframe and extract to new dataframe.\n",
    "    extracted_data.append(extracted_df) # append the extracted dataframe to a the list of extracted data.\n",
    "\n",
    "# Combine/concatinate all extracted DataFrames in extracted_data list into a single DataFrame called final_df (dataframe)\n",
    "final_df = pd.concat(extracted_data, ignore_index=True)\n",
    "\n",
    "# Ensure that no rows/papers are lost by countingfinal\n",
    "\n",
    "# calculate the number of papers before dataframes were\n",
    "preextraction_paper_count = 0               # define variable pre-extraction paper count and set to 0.\n",
    "for database, df in data.items():           # iterate through each database and corresponding dataframe in data dictionary\n",
    "    preextraction_paper_count += len(df)    # add the length of selected dataframe to the pre-extraction count\n",
    "\n",
    "# Calculate number of papers after concantination of desired columns from database dataframes.\n",
    "postextraction_paper_count = len(final_df)  # postextraction_paper_count = length of the final dataframe.\n",
    "\n",
    "# Print out result\n",
    "print(f\"There were {preextraction_paper_count} articles in the data before desired columns were extracted to a seperate dataframe. There are now {postextraction_paper_count} in the Final Dataframe. {preextraction_paper_count - postextraction_paper_count} papers were lost during this stage of processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format and Tidy Final Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reformat some journal information to aid removal of duplicates\n",
    "\n",
    "# fix capitalisation in Publication Title column\n",
    "final_df['Publication Title'] = final_df['Publication Title'].str.title()\n",
    "\n",
    "# sort by publication year, then first author\n",
    "final_df = final_df.sort_values(by=[\"Publication Year\", \"First Author\"]).reset_index(drop=True)\n",
    "\n",
    "# titlise first author column\n",
    "final_df[\"First Author\"] = final_df[\"First Author\"].str.title()\n",
    "\n",
    "# strip full stops from end of journal titles\n",
    "final_df[\"Title\"] = (\n",
    "    final_df[\"Title\"]\n",
    "        .str.strip(\".\")\n",
    "        .str.replace(r\"[()\\[\\]{}]\", \"\", regex=True)\n",
    "        .str.capitalize()\n",
    ")\n",
    "\n",
    "# remove word the from beginning of journal titles\n",
    "final_df[\"Publication Title\"] = final_df[\"Publication Title\"].str.replace(r\"^The\\s+\", \"\", regex=True)\n",
    "\n",
    "# reformat DOI to lower case\n",
    "final_df[\"DOI\"] = final_df[\"DOI\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'There are {paper_count_dict[0][\"Count\"][\"Start\"]} papers in the dataframe before duplicate removal.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate removal based on columns: Publication Year, First Author, Publication Title, Title, DOI: Start=360, End=360, Removed=0\n",
      "313 duplicates removed so far.\n",
      "Duplicate removal based on matching DOI: Start=360, End=360, Removed=0\n",
      "313 duplicates removed so far.\n"
     ]
    }
   ],
   "source": [
    "comparison_columns=[\"Publication Year\", \"First Author\", \"Publication Title\", \"Title\", \"DOI\"] # define comparison columns to compare duplicates against.\n",
    "final_df = final_df.drop_duplicates(subset=comparison_columns, keep=\"first\") # remove entries with same comparison column \n",
    "\n",
    "paper_count_dict = track_duplicate_removal(f\"Duplicate removal based on columns: {', '.join(comparison_columns)}\", len(final_df), tracking_dict=paper_count_dict) # update paper count dictionary\n",
    "\n",
    "final_df = final_df.drop_duplicates(subset=\"DOI\", keep=\"first\") # drop duplicate papers based on if they have matching DOI.\n",
    "paper_count_dict = track_duplicate_removal(f\"Duplicate removal based on matching DOI\", len(final_df), tracking_dict=paper_count_dict) # update paper count dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to CSV\n",
    "- Add three blank columns titled \"Exclude\", \"Reason Num\" and \"Reason\" to final_df for export as csv. \n",
    "- \"Exclude\" will be boolean variable (True/False) to mark whether paper is to be excluded or not.\n",
    "- \"Reason ID\" will be a numeric categorical variable will correspond to a table of exclusion criteria allowing for tracking an analysis of exclusion reasons, aiding PRISMA diagram.\n",
    "- \"Reason\" will contain a brief description or note of why the paper was included, likely the same as the text version of the Reason ID.\n",
    "\n",
    "Exclusion Criteria:\n",
    "1.\tFull text not available in English.\n",
    "2.\tStudies not focusing on intentional ingestion of foreign object via the oral cavity (mouth).\n",
    "3.\tStudies focussing solely on accidental ingestion.\n",
    "4.\tAnimal studies.\n",
    "5.\tReviews, editorials, commentaries, and opinion pieces without original empirical data.\n",
    "6.\tDuplicate publications or studies with overlapping data sets (the most comprehensive or recent study will be included).\n",
    "7.\tStudies focusing on ingestion of substances (e.g. poisons, medications) rather than physical foreign objects.\n",
    "8.\tIngestions undertaken in controlled environment as part of voluntary study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add three blank columns titled \"Exclude\", \"Reason Num\" and \"Reason\" to final_df for export as csv\n",
    "final_df[\"Exclude\"] = None      # Add blank \"Exclude\" column\n",
    "final_df[\"Reason ID\"] = None    # Add blank \"Reason ID\" column\n",
    "final_df[\"Reason\"] = None       # Add blank \"Reason\" column\n",
    "\n",
    "# Export to csv\n",
    "final_df.to_csv(\"/\".join([OUTPUT, \"title_abstract_review_start.csv\"]), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title and Abstract Screening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASReview\n",
    "This step was omitted as ASReview seemingly adds no benefit here.\n",
    "Majority of papers were deemed relevant and need to full-text review.\n",
    "All ASReview adds to this situation is using an AI model to suggest more relevant papers sooner and provides a nice user interface for use in the title and abstract screening process.\n",
    "Labelling articles as 'ASReview Relevant' or 'ASReview irrelevant' in ASReview attaches a note to the .ris file, which can then be read and interpreted in a reference manager. Whilst I intend to use a reference manager (Zotero), I would rather organise and label title and abstracts in Microsoft Excel, then later import the relevant papers (papers not excluded) to Zotero for full text screening. Using excel in this way also allows for categorising of exclusions which will benefit a PRISMA diagram.\n",
    "Can revisit this decision if things change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zotero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export as .ris to Zotero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIS file saved to /Users/jackgedge/Library/Mobile Documents/com~apple~CloudDocs/Dev/data/msc/dissertation/iifo_data/lit_search_4/op/output.ris\n"
     ]
    }
   ],
   "source": [
    "# Define the output RIS file path\n",
    "output_ris_file = \"/Users/jackgedge/Library/Mobile Documents/com~apple~CloudDocs/Dev/data/msc/dissertation/iifo_data/lit_search_4/op/output.ris\"\n",
    "\n",
    "# Convert DataFrame to RIS\n",
    "dataframe_to_ris(final_df, output_ris_file)\n",
    "\n",
    "print(f\"RIS file saved to {output_ris_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "- Expanding foam and other expanding chemicals were excluded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
